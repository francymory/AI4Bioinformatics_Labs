{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Caricamento dataset"
      ],
      "metadata": {
        "id": "l7YdIGL1pU9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv(\"/content/dataset_LUMINAL_A_B(in).csv\")\n",
        "# Separare features (gene expression) e labels (Luminal A/B)\n",
        "X = dataset.drop('l', axis=1).values  # Le espressioni geniche\n",
        "y = dataset['l'].values  # Le etichette (Luminal A/B)\n",
        "\n",
        "dataset.sample()\n",
        "X[0]\n",
        "y[0]\n",
        "#in X ci sono i nodi(righe) con le features/gene expressions(colonne)\n",
        "#in y ci sono le ground truths/labels per ogni nodo(riga)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hRIN4yD0lzw0",
        "outputId": "56ca0017-7bca-433d-ef6d-bdc12fb6ebce"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Luminal A    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing dati e labels"
      ],
      "metadata": {
        "id": "Sw241fKApZp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "#Le etichette vengono convertite in formato numerico (0,1) per poter essere utilizzate in modelli di machine learning."
      ],
      "metadata": {
        "id": "gHWtZhsvo4oQ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GNN: Creo il grafo, con gli edges, labels, features, costruisco GNN e la addestro e testo\n"
      ],
      "metadata": {
        "id": "O2aqhroRwIOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "#!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "#!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import dense_to_sparse\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# 1.Costruisco il Grafo con la Pearson Correlation\n",
        "\n",
        "# Calcolare la matrice di correlazione di Pearson tra i vettori di espressione genica dei pazienti (nodi)\n",
        "correlation_matrix = np.corrcoef(X)  # Correlazione tra righe (pazienti)\n",
        "# Stampa delle dimensioni del dataset e della matrice di correlazione\n",
        "print(f\"Dimensioni del dataset: {X.shape}\")\n",
        "print(f\"Dimensioni della matrice di correlazione: {correlation_matrix.shape}\")\n",
        "# Creiamo un edge solo se la correlazione tra due pazienti Ã¨ alta\n",
        "threshold = 0.5\n",
        "adjacency_matrix = (correlation_matrix > threshold).astype(int)\n",
        "# converto la matrice di adiacenza in edge_index (formato richiesto da PyTorch Geometric dove la prima riga contiene gli indici dei nodi di partenza degli edge, la seconda riga contiene gli indici dei nodi di arrivo degli edge.)\n",
        "edge_index, _ = dense_to_sparse(torch.tensor(adjacency_matrix))\n",
        "\n",
        "print(correlation_matrix)\n",
        "print(adjacency_matrix)\n",
        "print(edge_index)\n",
        "\n",
        "#2. Creo l'oggetto Data per PyTorch Geometric\n",
        "data = Data(\n",
        "    x=torch.tensor(X, dtype=torch.float32),  # Feature dei nodi (gene expression)\n",
        "    edge_index=edge_index,  # Gli edge costruiti dalla Pearson correlation\n",
        "    y=torch.tensor(y, dtype=torch.long)  # Le etichette (Luminal A/B)\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UcJc9ZEaq_UP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5634f1fb-f3b3-475b-f5e0-a1df973930f0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensioni del dataset: (100, 1022)\n",
            "Dimensioni della matrice di correlazione: (100, 100)\n",
            "[[1.         0.6540297  0.2532472  ... 0.18841422 0.43839602 0.20150986]\n",
            " [0.6540297  1.         0.78824131 ... 0.75353536 0.84955123 0.74069147]\n",
            " [0.2532472  0.78824131 1.         ... 0.99686232 0.76484103 0.99534994]\n",
            " ...\n",
            " [0.18841422 0.75353536 0.99686232 ... 1.         0.75260984 0.99711608]\n",
            " [0.43839602 0.84955123 0.76484103 ... 0.75260984 1.         0.74845195]\n",
            " [0.20150986 0.74069147 0.99534994 ... 0.99711608 0.74845195 1.        ]]\n",
            "[[1 1 0 ... 0 0 0]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [0 1 1 ... 1 1 1]\n",
            " ...\n",
            " [0 1 1 ... 1 1 1]\n",
            " [0 1 1 ... 1 1 1]\n",
            " [0 1 1 ... 1 1 1]]\n",
            "tensor([[ 0,  0,  0,  ..., 99, 99, 99],\n",
            "        [ 0,  1,  4,  ..., 97, 98, 99]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "797lqCzXwFk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import dense_to_sparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# 3. Definisco il Modello GCN\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Parametri del Modello\n",
        "input_dim = X.shape[1]  # Numero di feature (espressioni geniche)\n",
        "hidden_dim = 64  # Numero di neuroni nel layer nascosto\n",
        "output_dim = 2   # Numero di classi (Luminal A / Luminal B)\n",
        "\n",
        "# Inizializziamo il modello\n",
        "model = GCN(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# 4. Definire l'ottimizzatore e la funzione di loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# 5. Suddividere i dati in training e test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "train_mask = torch.tensor([i for i in range(len(X_train))], dtype=torch.long)\n",
        "test_mask = torch.tensor([i + len(X_train) for i in range(len(X_test))], dtype=torch.long)\n",
        "\n",
        "# Creare i tensor per train e test\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 6. Addestrare il modello GNN\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    out = model(data.x, data.edge_index)\n",
        "\n",
        "    # Calcolare la loss per il training\n",
        "    loss = criterion(out[train_mask], data.y[train_mask])\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# 7. Valutare il modello sul test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(data.x, data.edge_index)\n",
        "    pred = out.argmax(dim=1)\n",
        "\n",
        "    # Calcolare l'accuratezza\n",
        "    train_acc = accuracy_score(y_train, pred[:len(y_train)].cpu().numpy())\n",
        "    test_acc = accuracy_score(y_test, pred[len(y_train):].cpu().numpy())\n",
        "\n",
        "print(f'Train Accuracy: {train_acc:.4f}')\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlq3EM5fvVKX",
        "outputId": "cb5ec7c7-f489-40a0-b2fe-f04a698035e2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 2116887.5000\n",
            "Epoch [20/100], Loss: 978955.0000\n",
            "Epoch [30/100], Loss: 568127.1250\n",
            "Epoch [40/100], Loss: 76337.3906\n",
            "Epoch [50/100], Loss: 551377.4375\n",
            "Epoch [60/100], Loss: 189083.1875\n",
            "Epoch [70/100], Loss: 84465.4609\n",
            "Epoch [80/100], Loss: 134491.2188\n",
            "Epoch [90/100], Loss: 137690.4219\n",
            "Epoch [100/100], Loss: 65713.0391\n",
            "Train Accuracy: 0.5125\n",
            "Test Accuracy: 0.4500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confronto con MLP"
      ],
      "metadata": {
        "id": "iFPWgc7K79-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Definire il modello MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout per regolarizzare il modello\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return torch.log_softmax(x, dim=1)\n",
        "\n",
        "# Parametri del modello\n",
        "input_dim = X.shape[1]  # Numero di feature (espressioni geniche)\n",
        "hidden_dim = 64  # Numero di neuroni nei layer nascosti\n",
        "output_dim = 2   # Numero di classi (Luminal A / Luminal B)\n",
        "\n",
        "# Inizializzare il modello MLP\n",
        "mlp_model = MLP(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# 2. Definire l'ottimizzatore e la funzione di loss\n",
        "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "mlp_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 3. Suddividere i dati in training e test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convertire in tensor\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 4. Addestrare l'MLP\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    mlp_model.train()\n",
        "    mlp_optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    out = mlp_model(X_train_tensor)\n",
        "\n",
        "    # Calcolare la loss per il training\n",
        "    loss = mlp_criterion(out, y_train_tensor)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    mlp_optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# 5. Valutare l'MLP sul test set\n",
        "mlp_model.eval()\n",
        "with torch.no_grad():\n",
        "    out = mlp_model(X_test_tensor)\n",
        "    pred = out.argmax(dim=1)\n",
        "\n",
        "    # Calcolare l'accuratezza\n",
        "    test_acc = accuracy_score(y_test, pred)\n",
        "\n",
        "print(f'Test Accuracy (MLP): {test_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgu8sR6v79MV",
        "outputId": "9a5ec837-5d45-4b99-a66d-948f0e546672"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 26042.6504\n",
            "Epoch [20/100], Loss: 88.6739\n",
            "Epoch [30/100], Loss: 0.6830\n",
            "Epoch [40/100], Loss: 0.6844\n",
            "Epoch [50/100], Loss: 0.6922\n",
            "Epoch [60/100], Loss: 0.6911\n",
            "Epoch [70/100], Loss: 0.6834\n",
            "Epoch [80/100], Loss: 0.6835\n",
            "Epoch [90/100], Loss: 0.6835\n",
            "Epoch [100/100], Loss: 0.6919\n",
            "Test Accuracy (MLP): 0.4000\n"
          ]
        }
      ]
    }
  ]
}